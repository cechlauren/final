{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import *\n",
    "from NNfxns import neural_network\n",
    "#from data import rap1-lieb-positives\n",
    "from NNfxns import train\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training the neural net\n",
    "\n",
    "What can be used:\n",
    "     autoencoder\n",
    "     %%%%%%%%werk <partitions> <sampling>\n",
    "     test\n",
    "Arguments:\n",
    "    autoencoder\n",
    "        Run ze autoencoder\n",
    "    %%%%%%%%werk\n",
    "        Do the rap1 learning task including cross-validation\n",
    "    test\n",
    "        Classify test data and output to tsv file type\n",
    "    <partitions>\n",
    "        Number of partitions to make for cross-valitation (k-fold!)\n",
    "    <sampling>\n",
    "        Sampling method for NN training input\n",
    "        (slide) Go over each sequence in 17 nucleotide sliding frame (the length of the positives/test binding sites)\n",
    "        (space) Each sequence is cut into 17 nucleotide bits for inputs (the binary bits that are useable by our model)\n",
    "\"\"\"\n",
    "\n",
    "def werk():\n",
    "    \"\"\"\n",
    "    Train neural network on RAP1 binding sites\n",
    "        * Input layer with 17*4 nodes (because 17 different nucleotides defining each sequence, and 4 different\n",
    "          possible nucleotides describing those positions) + bias\n",
    "        * Hidden layer with 23-35 nodes (merge at least 2 input neurons)+ bias\n",
    "        * One output layer node (number of neurons in the output layer will equal the number of outputs\n",
    "          associated with each input; we want one answer)\n",
    "    Train against negative and positive binding sites\n",
    "        * Import all negative sequences from .fa file\n",
    "        * For each sequence, iterate every 17 bases and train with\n",
    "          expected of 0 (since these are negatives, while positives should be 1)\n",
    "        * Because it is so important to have a 1:1 ratio of negatives:positives when training our model, \n",
    "          for every 137 negative training instances, need to train it against all positive binding sites\n",
    "          with expected of 1 \n",
    "        * Will continue until negative binding sites have been fully ran through\n",
    "    \"\"\"\n",
    "\n",
    "    # This part takes care of bringing in all those positive sequences\n",
    "    \n",
    "    positive_sites = [pos_seq.strip() for pos_seq in open('data/rap1-lieb-positives.txt')]\n",
    "\n",
    "    \n",
    "    \n",
    "    # This part takes care of bringing in all those neg sequences\n",
    "    \n",
    "    negative_sites = list(SeqIO.parse('data/yeast-upstream-1k-negative.fa', 'fasta'))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Separate into random sections for the k-fold x-validation for both our positives and negatives \n",
    "    # Taken from : http://stackoverflow.com/questions/3352737/python-randomly-partition-a-list-into-n-nearly-equal-parts\n",
    "    \n",
    "    \n",
    "    partitions = int(args['<partitions>'])\n",
    "    neg_division = len(negative_sites) / float(partitions) # how many partitions we can make from the sites given\n",
    "    neg_randomly_partitioned_list = [negative_sites[int(round(neg_division * i)): int(round(neg_division * (i + 1)))]\n",
    "                                     for i in range(partitions)] # makes a list of those partitions thru each site\n",
    "\n",
    "    pos_division = len(positive_sites) / float(partitions) # ditto ^^\n",
    "    pos_randomly_partitioned_list = [positive_sites[int(round(pos_division * i)): int(round(pos_division * (i + 1)))]\n",
    "                                     for i in range(partitions)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Go thru neg sites subsets for x-validation, keep track of how many separations we do based on partitions\n",
    "    \n",
    "    separation = 0\n",
    "    for index in range(int(args['<partitions>'])):\n",
    "        # Set up cross-validation sets for the positives and negatives\n",
    "        neg_site_list_copy = copy.deepcopy(neg_randomly_partitioned_list)\n",
    "        del neg_site_list_copy[index]\n",
    "        neg_site_training = [seq for partition in neg_site_list_copy for seq in partition]\n",
    "        neg_cross_validation_set = neg_randomly_partitioned_list[index]\n",
    "\n",
    "        pos_site_list_copy = copy.deepcopy(pos_randomly_partitioned_list)\n",
    "        del pos_site_list_copy[index]\n",
    "        pos_site_training = [seq for partition in pos_site_list_copy for seq in partition]\n",
    "        pos_cross_validation_set = pos_randomly_partitioned_list[index]\n",
    "\n",
    "        print(\"Training on the training set...\")\n",
    "\n",
    "        # Input our hyperparameters = # nodes\n",
    "        NN = neural_network(68, 23, 1)\n",
    "\n",
    "        # See neural_net.py to get info on initialization\n",
    "        NN.initialize_values()\n",
    "\n",
    "        pos_counter = 0\n",
    "        counter = 0\n",
    "\n",
    "        # If we're sampling from tneg then we'll slide over 17 nucleotides \n",
    "        \n",
    "        if args['<sampling>'] == 'slide':\n",
    "            for site in neg_site_training:\n",
    "\n",
    "                # Iterate over site in 17 nucleotide sliding frames in negative sites, decide which model to use\n",
    "                for block in range(len(site) - 16):\n",
    "                    slice = site[block:(block + 17)].seq\n",
    "                    if slice not in positive_sites:\n",
    "                        if all([slice[4] == 'C', slice[5] == 'C', slice[9] == 'C']) == False:\n",
    "                            NN.set_input_and_expected_values(slice, autoencoder=False, negative=True)\n",
    "                            NN.forward_propogation()\n",
    "                            NN.backward_propogation()\n",
    "                            NN.update_weights_and_bias()\n",
    "                            pos_counter += 1\n",
    "                        else:\n",
    "                            print(slice)\n",
    "\n",
    "                    if pos_counter == len(pos_site_training):\n",
    "                        for pos_site in pos_site_training:\n",
    "                            NN.set_input_and_expected_values(pos_site, autoencoder=False, negative=False)\n",
    "                            NN.forward_propogation()\n",
    "                            NN.backward_propogation()\n",
    "                            NN.update_weights_and_bias()\n",
    "\n",
    "                        pos_counter = 0\n",
    "\n",
    "                # have reset the positives counter and will now say that we've done some training on those\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "                print(\"Training set: {}/{} completed...\".format(counter, len(neg_cross_validation_set)))\n",
    "\n",
    "                max_change_1 = NN.matrix_1_errors.max()\n",
    "                min_change_1 = NN.matrix_1_errors.min()\n",
    "                max_change_2 = NN.matrix_2_errors.max()\n",
    "                min_change_2 = NN.matrix_2_errors.min()\n",
    "\n",
    "                if any([max_change_1 < 0.00000000001 and max_change_1 > 0,\n",
    "                        min_change_1 > -.00000000001 and min_change_1 < 0]) and any(\n",
    "                    [max_change_2 < 0.00000000001 and max_change_2 > 0,\n",
    "                     min_change_2 > -0.00000000001 and min_change_2 < 0]):\n",
    "                    print(\"Stop criterion met after {} iterations\".format(counter))\n",
    "                    break\n",
    "\n",
    "        #when we sample from the negatives we only take 17 nucleotide chunks from each site\n",
    "        \n",
    "        if args['<sampling>'] == 'space':\n",
    "            for site in neg_site_training:\n",
    "                \n",
    "                number_of_blocks = int(len(site) / 17) #length of neg site tells us the amount of 17 length chunks possible\n",
    "\n",
    "                for block in range(number_of_blocks):\n",
    "                    slice = site[(block * 17):((block + 1) * 17)].seq\n",
    "                    if slice not in positive_sites:\n",
    "                        if all([slice[4] == 'C', slice[5] == 'C', slice[9] == 'C']) == False:\n",
    "                            NN.set_input_and_expected_values(slice, autoencoder=False, negative=True)\n",
    "                            NN.forward_propogation()\n",
    "                            NN.backward_propogation()\n",
    "                            NN.update_weights_and_bias()\n",
    "                            pos_counter += 1\n",
    "\n",
    "                        else:\n",
    "                            print(slice)\n",
    "\n",
    "                    #quick check to make sure that we've finished going thru the positives yet\n",
    "                    if pos_counter == len(pos_site_training):\n",
    "                        for pos_site in pos_site_training:\n",
    "                            NN.set_input_and_expected_values(pos_site, autoencoder=False, negative=False)\n",
    "                            NN.forward_propogation()\n",
    "                            NN.backward_propogation()\n",
    "                            NN.update_weights_and_bias()\n",
    "\n",
    "                        pos_counter = 0\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                max_change_1 = NN.matrix_1_errors.max()\n",
    "                min_change_1 = NN.matrix_1_errors.min()\n",
    "                max_change_2 = NN.matrix_2_errors.max()\n",
    "                min_change_2 = NN.matrix_2_errors.min()\n",
    "\n",
    "                if any([max_change_1 < 0.00000000001 and max_change_1 > 0,\n",
    "                        min_change_1 > -.00000000001 and min_change_1 < 0]) and any(\n",
    "                    [max_change_2 < 0.00000000001 and max_change_2 > 0,\n",
    "                     min_change_2 > -0.00000000001 and min_change_2 < 0]):\n",
    "                    print(\"Stop criterion met after {} iterations\".format(counter))\n",
    "                    break\n",
    "\n",
    "        # taken each partition and trained model\n",
    "        print(\"Performing Cross-validation\")\n",
    "\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "\n",
    "        \n",
    "        # Return the sets of positives and negatives from the x-validation\n",
    "        \n",
    "        print(\"Negative cross-validation set...\")\n",
    "        counter = 0\n",
    "        for site in neg_cross_validation_set:\n",
    "            for slice in range(len(site) - 16):\n",
    "                NN.set_input_and_expected_values(site[slice:slice + 17].seq, autoencoder=False, negative=True)\n",
    "                NN.forward_propogation()\n",
    "                neg_list.append(NN.output_layer_output)\n",
    "            counter += 1\n",
    "            print(\"Negative cross-validation: {}/{} completed...\".format(counter, len(neg_cross_validation_set)))\n",
    "            break\n",
    "\n",
    "        print(\"Positive cross-validation set...\")\n",
    "        for site in pos_cross_validation_set:\n",
    "            NN.set_input_and_expected_values(site, autoencoder=False)\n",
    "            NN.forward_propogation()\n",
    "            pos_list.append(NN.output_layer_output)\n",
    "\n",
    "        print('Positive avg: {}'.format(sum(pos_list) / len(pos_list)))\n",
    "        print('Negative avg: {}'.format(sum(neg_list) / len(neg_list)))\n",
    "        print(NN.matrix_1_bias)\n",
    "        print(NN.matrix_2_bias)\n",
    "\n",
    "        # Output the connection matrices with greatest separation between the average positive and negative scores\n",
    "        if ((sum(pos_list) / len(pos_list)) - (sum(neg_list) / len(neg_list))) > separation:\n",
    "            np.savetxt('connection_matrix_1.csv', NN.matrix_1_bias, delimiter=',')\n",
    "            np.savetxt('connection_matrix_2.csv', NN.matrix_2_bias, delimiter=',')\n",
    "            separation = (sum(pos_list) / len(pos_list)) - (sum(neg_list) / len(neg_list))\n",
    "\n",
    "\n",
    "# A simple definition of the autoencoder that uses those same NN parameters\n",
    "def autoencoder():\n",
    "  \n",
    "    NN = neural_network()\n",
    "    NN.set_input_and_expected_values('GA', autoencoder=True)\n",
    "    NN.initialize_values()\n",
    "\n",
    "    # Stop criterion\n",
    "    finished_working = False\n",
    "\n",
    "    while finished_working == False:\n",
    "        NN.forward_propogation()\n",
    "        NN.backward_propogation()\n",
    "        NN.update_weights_and_bias()\n",
    "\n",
    "        max_change_1 = NN.matrix_1_errors.max()\n",
    "        min_change_1 = NN.matrix_1_errors.min()\n",
    "        max_change_2 = NN.matrix_2_errors.max()\n",
    "        min_change_2 = NN.matrix_2_errors.min()\n",
    "\n",
    "        if any([max_change_1 < 0.00001 and max_change_1 > 0,\n",
    "                min_change_1 > -.00001 and min_change_1 < 0]) or any(\n",
    "            [max_change_2 < 0.00001 and max_change_2 > 0,\n",
    "             min_change_2 > -0.00001 and min_change_2 < 0]):\n",
    "            finished_working = True\n",
    "\n",
    "    print(NN.output_layer_output)\n",
    "\n",
    "def test():\n",
    "    test_sequences = open('data/eightBit.txt')\n",
    "    NN = auto()\n",
    "    NN.matrix_1_bias = np.loadtxt('connection_matrix_1.csv', delimiter=',')\n",
    "    NN.matrix_2_bias = np.loadtxt('connection_matrix_2.csv', delimiter=',')\n",
    "\n",
    "    NN_outputs = open('NN_predictions.txt', 'w')\n",
    "\n",
    "    for test_seq in test_sequences:\n",
    "        NN.set_input_and_expected_values(test_seq.strip())\n",
    "        NN.forward_propogation()\n",
    "        NN_outputs.write('{}\\t{}\\n'.format(test_seq.strip(), NN.output_layer_output[0]))\n",
    "\n",
    "    NN_outputs.close()\n",
    "\n",
    "if __name__ == 'train':\n",
    "    import docopt\n",
    "    import numpy as np\n",
    "    from Bio import SeqIO\n",
    "    import copy\n",
    "    from .NNfxns import neural_network\n",
    "\n",
    "    args = docopt.docopt(__doc__)\n",
    "\n",
    "    if args['autoencoder']:\n",
    "        autoencoder()\n",
    "\n",
    "    if args['werk']:\n",
    "        werk()\n",
    "\n",
    "    if args['test']:\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "-Make a NN class\n",
    "-define my functions\n",
    "  -autoencoder and its parameters\n",
    "  -initial values/input\n",
    "  -vecotorization\n",
    "  -expected values\n",
    "  -output\n",
    "  -backwards propagation\n",
    "  -feedforward propagation\n",
    "  -weights/bias\n",
    "  -sigmoid function\n",
    "  -sigmoid derivative\n",
    "\n",
    "\"\"\"\n",
    "# Make a class to allow self reference\n",
    "class auto():\n",
    "\n",
    "# The NN will have a number of input neurons, a hidden layer, and number of an output neurons of the specified numbers, \n",
    "# which we apply to the class and can adjust as necessary later\n",
    "\n",
    "    def __init__(self, input_layer_nodes=8, hidden_layer_nodes=3, output_layer_nodes=8):\n",
    "        self.input_layer_nodes = input_layer_nodes\n",
    "        self.hidden_layer_nodes = hidden_layer_nodes\n",
    "        self.output_layer_nodes = output_layer_nodes\n",
    "\n",
    "        # We want the autoencoder to learn the best weights for the data so we \n",
    "        # initialize matrices with random weights 0.1 > x > -0.1\n",
    "        # These will link the autoencoder's different layers.\n",
    "        \n",
    "        self.connection_matrix_1 = np.random.randn(self.input_layer_nodes, self.hidden_layer_nodes) / 10\n",
    "        self.connection_matrix_2 = np.random.randn(self.hidden_layer_nodes, self.output_layer_nodes) / 10\n",
    "\n",
    "        # We have several vectors that our encoder will make to take in (input) and put out (output) data.\n",
    "        # Recall the hidden layer will reduce the dimension, in a vector-wise fashion\n",
    "        \n",
    "        self.input_vector = None\n",
    "        self.hidden_layer_output = None\n",
    "        self.output_layer_output = None\n",
    "\n",
    "        # We can also start off our autoencoder with a biased vector or matrices, depending on what we need from our model\n",
    "        \n",
    "        self.input_with_bias = None\n",
    "        self.matrix_1_bias = None\n",
    "        self.matrix_2_bias = None\n",
    "\n",
    "        # Recall that we need to take the derivative of the cost function with respect to weight, and develop matrices from this\n",
    "        \n",
    "        self.hidden_dx_matrix = np.zeros([self.hidden_layer_nodes, self.hidden_layer_nodes])\n",
    "        self.output_dx_matrix = np.zeros([self.output_layer_nodes, self.output_layer_nodes])\n",
    "\n",
    "        # Hyperparameter: Learning Rate (controls how quickly a NN learns a problem)\n",
    "        # Typically in a range between [0.1, 1], configureable\n",
    "        # A perfect learning rate will make the model learn to best approximate the function given available resources (layers/nodes)\n",
    "        # in a given number of epochs\n",
    "        # We'll start with a large one because we've got other stuff to do, dudes. The pitfall of that is that it will likely arrive at a suboptimal\n",
    "        # set of weights.\n",
    "        \n",
    "        self.learning_rate = 1\n",
    "\n",
    "        \n",
    "        \n",
    "        # Bit conversion for DNA into NN inputs because I'm too lazy to think of a different way to encode these. Recall, we want either 0 or 1, not anything continuous.\n",
    "        \n",
    "        self.base_binary_conversion = {'0': '0',\n",
    "                                       '1': '1'\n",
    "                                       }\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # These are the expected values for our NN to return\n",
    "        \n",
    "        self.expected_values = None\n",
    "\n",
    "\n",
    "\n",
    "    # Now we make moves to fill in and interpret those matrices\n",
    "    \n",
    "    # Initialize the values of the bias matrix given the information passing through the hidden layer and output layer\n",
    "    \n",
    "    def initialize_values(self):\n",
    "        bias_ones_1 = np.ones([1, self.hidden_layer_nodes])\n",
    "        self.matrix_1_bias = np.append(self.connection_matrix_1, bias_ones_1, axis=0)\n",
    "\n",
    "        bias_ones_2 = np.ones([1, self.output_layer_nodes])\n",
    "        self.matrix_2_bias = np.append(self.connection_matrix_2, bias_ones_2, axis=0)\n",
    "\n",
    "    # We'll make the binding sites into binary bits that the encoder can interpret, and then tell it what to expect \n",
    "    \n",
    "    def set_input_and_expected_values(self, input_DNA, autoencoder=True, negative=True):\n",
    "        # Convert input DNA sequence into binary bits\n",
    "        self._construct_input_vector(input_DNA)\n",
    "        # Set expected value depending on autoencoder or werk\n",
    "        self._set_expected_values(autoencoder, negative)\n",
    "        # Weight matrices and input/output vectors with bias applied\n",
    "        self.input_with_bias = np.append(self.input_vector, [1])\n",
    "\n",
    "    \n",
    "    \n",
    "    # This directly handles conversion of the DNA binding site string to the 1/0 vector describing it, and assigns it to the original input class\n",
    "    \n",
    "    def _construct_input_vector(self, input_DNA):\n",
    "       \n",
    "        temp_vector_list = []\n",
    "\n",
    "        for base in input_DNA:\n",
    "            for number in self.base_binary_conversion[base]:\n",
    "                temp_vector_list.append(float(number))\n",
    "\n",
    "        self.input_vector = np.asarray(temp_vector_list)\n",
    "\n",
    "    \n",
    "    \n",
    "    # This will set the values we expect from the NN depending on whether or not we use the autoencoder (T/F).\n",
    "    \n",
    "    def _set_expected_values(self, autoencoder=True, negative=True):\n",
    "        if autoencoder == True:\n",
    "            self.expected_values = self.input_vector\n",
    "\n",
    "        if autoencoder == False:\n",
    "            if negative == True:\n",
    "                self.expected_values = 0\n",
    "            if negative == False:\n",
    "                self.expected_values = 1\n",
    "\n",
    "    # Recall an autoencoder is a feedforward method. We need to convert input to hidden layer reduced dim info to output layer\n",
    "    # that is the same as the input.\n",
    "    \n",
    "    def forward_propogation(self):\n",
    "        # Generates hidden layer outputs\n",
    "        \n",
    "        output_one_list = []\n",
    "\n",
    "        for element in np.nditer(np.dot(self.input_with_bias, self.matrix_1_bias)):\n",
    "            output_one_list.append(self._sigmoid_function(element))\n",
    "        self.hidden_layer_output = np.asarray(output_one_list)\n",
    "\n",
    "        # Calculate the square derivate matrix for the hidden layer outputs\n",
    "        \n",
    "        for position, value in enumerate(self.hidden_layer_output):\n",
    "            self.hidden_dx_matrix[position][position] = self._sigmoid_function_derivative(value)\n",
    "\n",
    "        # The results from the output layer \n",
    "        # Add bias to hidden_layer_output\n",
    "        \n",
    "        self.hidden_output_bias = np.append(self.hidden_layer_output, [1])\n",
    "\n",
    "        output_two_list = []\n",
    "        for element in np.nditer(np.dot(self.hidden_output_bias, self.matrix_2_bias)):\n",
    "            output_two_list.append(self._sigmoid_function(element))\n",
    "        self.output_layer_output = np.asarray(output_two_list)\n",
    "\n",
    "        # Calculate square derivate matrix for output layer outputs\n",
    "        \n",
    "        for position, value in enumerate(self.output_layer_output):\n",
    "            self.output_dx_matrix[position][position] = self._sigmoid_function_derivative(value)\n",
    "\n",
    "    \n",
    "    # Recall that in stochastic gradient descent, we estimate the error gradient for the current state of the model using\n",
    "    # examples from the training. The backwards propagation is what we use to update the weights of the model. \n",
    "    \n",
    "    def backward_propogation(self):\n",
    "        # Output Layer error\n",
    "        \n",
    "        deviations = self.output_layer_output - self.expected_values\n",
    "\n",
    "        output_layer_errors = np.dot(self.output_dx_matrix, deviations)\n",
    "\n",
    "        # Hidden Layer error\n",
    "        \n",
    "        hidden_layer_errors = np.dot(np.dot(self.hidden_dx_matrix, self.connection_matrix_2), output_layer_errors)\n",
    "\n",
    "        # Matrix 2 Errors (those of our output layer)\n",
    "        \n",
    "        output_rated_row_vector = np.asmatrix(-(self.learning_rate * output_layer_errors)).transpose()\n",
    "        matrix_2_errors_transposed = np.dot(output_rated_row_vector, np.asmatrix(self.hidden_output_bias))\n",
    "        self.matrix_2_errors = matrix_2_errors_transposed.transpose()\n",
    "\n",
    "        # Matrix 1 Errors (those of our hidden layer)\n",
    "        \n",
    "        hidden_rated_row_vector = np.asmatrix(-(self.learning_rate * hidden_layer_errors)).transpose()\n",
    "        matrix_1_errors_transposed = np.dot(hidden_rated_row_vector, np.asmatrix(self.input_with_bias))\n",
    "        self.matrix_1_errors = matrix_1_errors_transposed.transpose()\n",
    "\n",
    "    \n",
    "    # Basically the sum between the output and hidden layers' bias and errors is what we use...\n",
    "    \n",
    "    def update_weights_and_bias(self):\n",
    "        self.matrix_1_bias = self.matrix_1_bias + self.matrix_1_errors\n",
    "        self.matrix_2_bias = self.matrix_2_bias + self.matrix_2_errors\n",
    "\n",
    "        self.connection_matrix_1 = self.matrix_1_bias[:-1]\n",
    "        self.connection_matrix_2 = self.matrix_2_bias[:-1]\n",
    "\n",
    "    \n",
    "    # The activation function for the layers. Some people use relu. Its more of less simple to take the deriv of sigmoid. \n",
    "    \n",
    "    def _sigmoid_function(self, input):\n",
    "        return float(1 / (1 + np.exp(-input)))\n",
    "\n",
    "    \n",
    "    # Recall we also take the derivative of the activatio function. \n",
    "    def _sigmoid_function_derivative(self, input):\n",
    "        return float(self._sigmoid_function(input) * (1 - self._sigmoid_function(input)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_auto():\n",
    "  \n",
    "    eightBit = open('data/eightBit.txt')\n",
    "    NN = auto()\n",
    "    for bits in eightBit:\n",
    "        NN.set_input_and_expected_values(bits.strip(), autoencoder=True)\n",
    "        NN.initialize_values()\n",
    "        \n",
    "        # Stop criterion\n",
    "        finished_working = False\n",
    "\n",
    "        while finished_working == False:\n",
    "            NN.forward_propogation()\n",
    "            NN.backward_propogation()\n",
    "            NN.update_weights_and_bias()\n",
    "\n",
    "            max_change_1 = NN.matrix_1_errors.max()\n",
    "            min_change_1 = NN.matrix_1_errors.min()\n",
    "            max_change_2 = NN.matrix_2_errors.max()\n",
    "            min_change_2 = NN.matrix_2_errors.min()\n",
    "\n",
    "            if any([max_change_1 < 0.00001 and max_change_1 > 0,\n",
    "                    min_change_1 > -.00001 and min_change_1 < 0]) or any(\n",
    "                [max_change_2 < 0.00001 and max_change_2 > 0,\n",
    "                 min_change_2 > -0.00001 and min_change_2 < 0]):\n",
    "                finished_working = True\n",
    "\n",
    "        print(NN.output_layer_output)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='data/eightBit.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "print(open('data/eightBit.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99949126e-01 3.99995008e-05 3.99955293e-05 3.99962867e-05\n",
      " 3.99984625e-05 3.99967960e-05 3.99960781e-05 3.99999973e-05]\n",
      "[3.99985675e-05 9.99946848e-01 2.57926418e-05 2.60418918e-05\n",
      " 2.59730002e-05 2.59787067e-05 2.59901803e-05 2.61870126e-05]\n",
      "[3.01333250e-05 3.99988046e-05 9.99939176e-01 1.49154779e-05\n",
      " 1.51149238e-05 1.49429129e-05 1.48778409e-05 1.52154691e-05]\n",
      "[1.93195523e-05 2.48948966e-05 3.99988031e-05 9.99929688e-01\n",
      " 1.03933983e-05 1.03922915e-05 1.03475381e-05 1.02865919e-05]\n",
      "[0.00193798 0.00299085 0.00481673 0.00698593 0.9829648  0.00125429\n",
      " 0.0012227  0.00112854]\n",
      "[9.27584501e-06 1.31990155e-05 1.80325124e-05 3.99995084e-05\n",
      " 1.97193982e-05 9.99924754e-01 5.72898531e-06 5.85558823e-06]\n",
      "[8.00878183e-06 1.04457314e-05 1.24600157e-05 1.38026387e-05\n",
      " 1.72800042e-05 3.99992599e-05 9.99930160e-01 6.26102287e-06]\n",
      "[4.22246876e-06 5.26472532e-06 6.90258785e-06 8.73540765e-06\n",
      " 9.41536990e-06 1.97718351e-05 3.99987067e-05 9.99918081e-01]\n"
     ]
    }
   ],
   "source": [
    "test_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this when determining how well the nn gets scores for the test set\n",
    "def test():\n",
    "    test_sequences = open('data/rap1-lieb-test.txt')\n",
    "    NN = neural_network(68, 23, 1)\n",
    "    NN.matrix_1_bias = np.loadtxt('connection_matrix_1.csv', delimiter=',')\n",
    "    NN.matrix_2_bias = np.loadtxt('connection_matrix_2.csv', delimiter=',')\n",
    "\n",
    "    NN_outputs = open('NN_predictions.txt', 'w')\n",
    "\n",
    "    for test_seq in test_sequences:\n",
    "        NN.set_input_and_expected_values(test_seq.strip())\n",
    "        NN.forward_propogation()\n",
    "        NN_outputs.write('{}\\t{}\\n'.format(test_seq.strip(), NN.output_layer_output[0]))\n",
    "\n",
    "    NN_outputs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
