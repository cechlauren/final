{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run first\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "import docopt\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import copy\n",
    "from NNfxns import neural_network\n",
    "from NNfxns import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run second\n",
    "\n",
    "\"\"\"\n",
    "-Make a neuralnet class\n",
    "-define my functions\n",
    "  -autoencoder and its parameters\n",
    "  -initial values/input\n",
    "  -vecotorization\n",
    "  -expected values\n",
    "  -output\n",
    "  -backwards propagation\n",
    "  -feedforward propagation\n",
    "  -weights/bias\n",
    "  -sigmoid function\n",
    "  -sigmoid derivative\n",
    "\n",
    "\"\"\"\n",
    "# Make a class to allow self reference\n",
    "class neural_network():\n",
    "\n",
    "# The neuralnet will have a number of input neurons, a hidden layer, and number of an output neurons of the specified numbers, \n",
    "# which we apply to the class and can adjust as necessary later\n",
    "\n",
    "    def __init__(self, input_neur=8, hidden_neur=3, output_neur=8):\n",
    "        self.input_neur = input_neur\n",
    "        self.hidden_neur = hidden_neur\n",
    "        self.output_neur = output_neur\n",
    "\n",
    "        # We want the autoencoder to learn the best weights for the data so we \n",
    "        # initialize matrices with random weights 0.1 > x > -0.1\n",
    "        # These will link the autoencoder's different layers.\n",
    "        \n",
    "        self.cnx_matrix_1 = np.random.randn(self.input_neur, self.hidden_neur) / 10\n",
    "        self.cnx_matrix_2 = np.random.randn(self.hidden_neur, self.output_neur) / 10\n",
    "\n",
    "        # We have several vectors that our encoder will make to take in (input) and put out (output) data.\n",
    "        # Recall the hidden layer will reduce the dimension, in a vector-wise fashion\n",
    "        \n",
    "        self.input_vector = None\n",
    "        self.hidden_neur_output = None\n",
    "        self.out_neur_output = None\n",
    "\n",
    "        # We can also start off our autoencoder with a biased vector or matrices, depending on what we need from our model\n",
    "        \n",
    "        self.input_with_bias = None\n",
    "        self.bias_mat1 = None\n",
    "        self.bias_mat2 = None\n",
    "\n",
    "        # Recall that we need to take the derivative of the cost function with respect to weight, and develop matrices from this\n",
    "        \n",
    "        self.hidden_dx_matrix = np.zeros([self.hidden_neur, self.hidden_neur])\n",
    "        self.output_dx_matrix = np.zeros([self.output_neur, self.output_neur])\n",
    "\n",
    "        # Hyperparameter: Learning Rate (controls how quickly a neuralnet learns a problem)\n",
    "        # Typically in a range between [0.1, 1], configureable\n",
    "        # A perfect learning rate will make the model learn to best approximate the function given available resources (layers/nodes)\n",
    "        # in a given number of epochs\n",
    "        # We'll start with a large one because we've got other stuff to do, dudes. The pitfall of that is that it will likely arrive at a suboptimal\n",
    "        # set of weights.\n",
    "        \n",
    "        self.learning_rate = 1\n",
    "\n",
    "        \n",
    "        \n",
    "        # Bit conversion for DNA into neuralnet inputs because I'm too lazy to think of a different way to encode these. Recall, we want either 0 or 1, not anything continuous.\n",
    "        \n",
    "        self.make_mulah = {'A': '0001',\n",
    "                                       'C': '0010',\n",
    "                                       'T': '0100',\n",
    "                                       'G': '1000'\n",
    "                                       }\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # These are the expected values for our neuralnet to return\n",
    "        \n",
    "        self.expected_values = None\n",
    "\n",
    "\n",
    "\n",
    "    # Now we make moves to fill in and interpret those matrices\n",
    "    \n",
    "    # Initialize the values of the bias matrix given the information passing through the hidden layer and output layer\n",
    "    \n",
    "    def initialize_values(self):\n",
    "        bias_ones_1 = np.ones([1, self.hidden_neur])\n",
    "        self.bias_mat1 = np.append(self.cnx_matrix_1, bias_ones_1, axis=0)\n",
    "\n",
    "        bias_ones_2 = np.ones([1, self.output_neur])\n",
    "        self.bias_mat2 = np.append(self.cnx_matrix_2, bias_ones_2, axis=0)\n",
    "\n",
    "    # We'll make the binding sites into binary bits that the encoder can interpret, and then tell it what to expect \n",
    "    \n",
    "    def setin_n_exp_values(self, dnaIN, autoencoder=True, negative=True):\n",
    "        # Convert input DNA sequence into binary bits\n",
    "        self._construct_input_vector(dnaIN)\n",
    "        # Set expected value depending on autoencoder or testme\n",
    "        self._set_expected_values(autoencoder, negative)\n",
    "        # Weight matrices and input/output vectors with bias applied\n",
    "        self.input_with_bias = np.append(self.input_vector, [1])\n",
    "\n",
    "    \n",
    "    \n",
    "    # This directly handles conversion of the DNA binding site string to the 1/0 vector describing it, and assigns it to the original input class\n",
    "    \n",
    "    def _construct_input_vector(self, dnaIN):\n",
    "       \n",
    "        temp_vector_list = []\n",
    "\n",
    "        for base in dnaIN:\n",
    "            for number in self.make_mulah[base]:\n",
    "                temp_vector_list.append(float(number))\n",
    "\n",
    "        self.input_vector = np.asarray(temp_vector_list)\n",
    "\n",
    "    \n",
    "    \n",
    "    # This will set the values we expect from the neuralnet depending on whether or not we use the autoencoder (T/F).\n",
    "    \n",
    "    def _set_expected_values(self, autoencoder=True, negative=True):\n",
    "        if autoencoder == True:\n",
    "            self.expected_values = self.input_vector\n",
    "\n",
    "        if autoencoder == False:\n",
    "            if negative == True:\n",
    "                self.expected_values = 0\n",
    "            if negative == False:\n",
    "                self.expected_values = 1\n",
    "\n",
    "    # Recall an autoencoder is a feedforward method. We need to convert input to hidden layer reduced dim info to output layer\n",
    "    # that is the same as the input.\n",
    "    \n",
    "    def forward_propogation(self):\n",
    "        # Generates hidden layer outputs\n",
    "        \n",
    "        output_one_list = []\n",
    "\n",
    "        for element in np.nditer(np.dot(self.input_with_bias, self.bias_mat1)):\n",
    "            output_one_list.append(self._sigmoid_function(element))\n",
    "        self.hidden_neur_output = np.asarray(output_one_list)\n",
    "\n",
    "        # Calculate the square derivate matrix for the hidden layer outputs\n",
    "        \n",
    "        for position, value in enumerate(self.hidden_neur_output):\n",
    "            self.hidden_dx_matrix[position][position] = self._sigmoid_function_derivative(value)\n",
    "\n",
    "        # The results from the output layer \n",
    "        # Add bias to hidden_neur_output\n",
    "        \n",
    "        self.hidden_output_bias = np.append(self.hidden_neur_output, [1])\n",
    "\n",
    "        output_two_list = []\n",
    "        for element in np.nditer(np.dot(self.hidden_output_bias, self.bias_mat2)):\n",
    "            output_two_list.append(self._sigmoid_function(element))\n",
    "        self.out_neur_output = np.asarray(output_two_list)\n",
    "\n",
    "        # Calculate square derivate matrix for output layer outputs\n",
    "        \n",
    "        for position, value in enumerate(self.out_neur_output):\n",
    "            self.output_dx_matrix[position][position] = self._sigmoid_function_derivative(value)\n",
    "\n",
    "    \n",
    "    # Recall that in stochastic gradient descent, we estimate the error gradient for the current state of the model using\n",
    "    # examples from the training. The backwards propagation is what we use to update the weights of the model. \n",
    "    \n",
    "    def backward_propogation(self):\n",
    "        # Output Layer error\n",
    "        \n",
    "        deviations = self.out_neur_output - self.expected_values\n",
    "\n",
    "        out_neur_errors = np.dot(self.output_dx_matrix, deviations)\n",
    "\n",
    "        # Hidden Layer error\n",
    "        \n",
    "        hidden_neur_errors = np.dot(np.dot(self.hidden_dx_matrix, self.cnx_matrix_2), out_neur_errors)\n",
    "\n",
    "        # Matrix 2 Errors (those of our output layer)\n",
    "        \n",
    "        output_rated_row_vector = np.asmatrix(-(self.learning_rate * out_neur_errors)).transpose()\n",
    "        errors_mat2_transposed = np.dot(output_rated_row_vector, np.asmatrix(self.hidden_output_bias))\n",
    "        self.errors_mat2 = errors_mat2_transposed.transpose()\n",
    "\n",
    "        # Matrix 1 Errors (those of our hidden layer)\n",
    "        \n",
    "        hidden_rated_row_vector = np.asmatrix(-(self.learning_rate * hidden_neur_errors)).transpose()\n",
    "        errors_mat1_transposed = np.dot(hidden_rated_row_vector, np.asmatrix(self.input_with_bias))\n",
    "        self.errors_mat1 = errors_mat1_transposed.transpose()\n",
    "\n",
    "    \n",
    "    # Basically the sum between the output and hidden layers' bias and errors is what we use...\n",
    "    \n",
    "    def weight_bias_renew(self):\n",
    "        self.bias_mat1 = self.bias_mat1 + self.errors_mat1\n",
    "        self.bias_mat2 = self.bias_mat2 + self.errors_mat2\n",
    "\n",
    "        self.cnx_matrix_1 = self.bias_mat1[:-1]\n",
    "        self.cnx_matrix_2 = self.bias_mat2[:-1]\n",
    "\n",
    "    \n",
    "    # The activation function for the layers. Some people use relu. Its more of less simple to take the deriv of sigmoid. \n",
    "    \n",
    "    def _sigmoid_function(self, input):\n",
    "        return float(1 / (1 + np.exp(-input)))\n",
    "\n",
    "    \n",
    "    # Recall we also take the derivative of the activatio function. \n",
    "    def _sigmoid_function_derivative(self, input):\n",
    "        return float(self._sigmoid_function(input) * (1 - self._sigmoid_function(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run third\n",
    "\"\"\"\n",
    "Training the neural net\n",
    "\n",
    "What can be used:\n",
    "     autoencoder\n",
    "     testme <splits> <sampling>\n",
    "     test\n",
    "Arguments:\n",
    "    autoencoder\n",
    "        Run ze autoencoder\n",
    "    testme\n",
    "        Do the rap1 learning task including cross-validation\n",
    "    test\n",
    "        Classify test data and output to tsv file type\n",
    "    <splits>\n",
    "        Number of splits to make for cross-valitation (k-fold!)\n",
    "    <sampling>\n",
    "        Sampling method for neuralnet training input\n",
    "        (slide) Go over each sequence in 17 nucleotide sliding frame (the length of the positives/test binding sites)\n",
    "        (space) Each sequence is cut into 17 nucleotide bits for inputs (the binary bits that are useable by our model)\n",
    "\"\"\"\n",
    "\n",
    "def testme():\n",
    "    \"\"\"\n",
    "    Train neural network on RAP1 binding sites\n",
    "        * Input layer with 17*4 nodes (because 17 different nucleotides defining each sequence, and 4 different\n",
    "          possible nucleotides describing those positions) + bias\n",
    "        * Hidden layer with 23-35 nodes (merge at least 2 input neurons)+ bias\n",
    "        * One output layer node (number of neurons in the output layer will equal the number of outputs\n",
    "          associated with each input; we want one answer)\n",
    "    Train against negative and positive binding sites\n",
    "        * Import all negative sequences from .fa file\n",
    "        * For each sequence, iterate every 17 bases and train with\n",
    "          expected of 0 (since these are negatives, while positives should be 1)\n",
    "        * Because it is so important to have a 1:1 ratio of negatives:positives when training our model, \n",
    "          for every 137 negative training instances, need to train it against all positive binding sites\n",
    "          with expected of 1 \n",
    "        * Will continue until negative binding sites have been fully ran through\n",
    "    \"\"\"\n",
    "\n",
    "    # This part takes care of bringing in all those positive sequences\n",
    "    \n",
    "    pos = [pos_sequence.strip() for pos_sequence in open('data/rap1-lieb-positives.txt')]\n",
    "\n",
    "    \n",
    "    \n",
    "    # This part takes care of bringing in all those neg sequences\n",
    "    \n",
    "    neg = list(SeqIO.parse('data/yeast-upstream-1k-negative.fa', 'fasta'))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Separate into random sections for the k-fold x-validation for both our positives and negatives \n",
    "    # Taken from : http://stackoverflow.com/questions/3352737/python-randomly-partition-a-list-into-n-nearly-equal-parts\n",
    "    \n",
    "    \n",
    "    splits = int(args['<splits>'])\n",
    "    negative_division = len(neg) / float(splits) # how many splits we can make from the sites given\n",
    "    neg_split_up = [neg[int(round(negative_division * i)): int(round(negative_division * (i + 1)))]\n",
    "                                     for i in range(splits)] # makes a list of those splits thru each site\n",
    "\n",
    "    pos_division = len(pos) / float(splits) # ditto ^^\n",
    "    pos_split_up = [pos[int(round(pos_division * i)): int(round(pos_division * (i + 1)))]\n",
    "                                     for i in range(splits)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Go thru neg sites subsets for x-validation, keep track of how many separations we do based on splits\n",
    "    \n",
    "    separation = 0\n",
    "    for index in range(int(args['<splits>'])):\n",
    "        # Set up cross-validation sets for the positives and negatives\n",
    "        neg_site_list_copy = copy.deepcopy(neg_split_up)\n",
    "        del neg_site_list_copy[index]\n",
    "        neg_site_training = [seq for partition in neg_site_list_copy for seq in partition]\n",
    "        neg_cross_validation_set = neg_split_up[index]\n",
    "\n",
    "        pos_site_list_copy = copy.deepcopy(pos_split_up)\n",
    "        del pos_site_list_copy[index]\n",
    "        pos_site_training = [seq for partition in pos_site_list_copy for seq in partition]\n",
    "        pos_cross_validation_set = pos_split_up[index]\n",
    "\n",
    "        print(\"Training on the training set...\")\n",
    "\n",
    "        # Input our hyperparameters = # nodes\n",
    "        neuralnet = neural_network(68, 23, 1)\n",
    "\n",
    "        # See neural_net.py to get info on initialization\n",
    "        neuralnet.initialize_values()\n",
    "\n",
    "        pos_counter = 0\n",
    "        counter = 0\n",
    "\n",
    "        # If we're sampling from tneg then we'll slide over 17 nucleotides \n",
    "        \n",
    "        if args['<sampling>'] == 'slide':\n",
    "            for site in neg_site_training:\n",
    "\n",
    "                # Iterate over site in 17 nucleotide sliding frames in negative sites, decide which model to use\n",
    "                for chunky in range(len(site) - 16):\n",
    "                    slice = site[chunky:(chunky + 17)].seq\n",
    "                    if slice not in pos:\n",
    "                        if all([slice[4] == 'C', slice[5] == 'C', slice[9] == 'C']) == False:\n",
    "                            neuralnet.setin_n_exp_values(slice, autoencoder=False, negative=True)\n",
    "                            neuralnet.forward_propogation()\n",
    "                            neuralnet.backward_propogation()\n",
    "                            neuralnet.weight_bias_renew()\n",
    "                            pos_counter += 1\n",
    "                        else:\n",
    "                            print(slice)\n",
    "\n",
    "                    if pos_counter == len(pos_site_training):\n",
    "                        for pos_site in pos_site_training:\n",
    "                            neuralnet.setin_n_exp_values(pos_site, autoencoder=False, negative=False)\n",
    "                            neuralnet.forward_propogation()\n",
    "                            neuralnet.backward_propogation()\n",
    "                            neuralnet.weight_bias_renew()\n",
    "\n",
    "                        pos_counter = 0\n",
    "\n",
    "                # have reset the positives counter and will now say that we've done some training on those\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "                print(\"Training set: {}/{} completed...\".format(counter, len(neg_cross_validation_set)))\n",
    "\n",
    "                greatestdelta_1 = neuralnet.errors_mat1.max()\n",
    "                smallestdelta_1 = neuralnet.errors_mat1.min()\n",
    "                greatestdelta_2 = neuralnet.errors_mat2.max()\n",
    "                smallestdelta_2 = neuralnet.errors_mat2.min()\n",
    "\n",
    "                if any([greatestdelta_1 < 0.00000000001 and greatestdelta_1 > 0,\n",
    "                        smallestdelta_1 > -.00000000001 and smallestdelta_1 < 0]) and any(\n",
    "                    [greatestdelta_2 < 0.00000000001 and greatestdelta_2 > 0,\n",
    "                     smallestdelta_2 > -0.00000000001 and smallestdelta_2 < 0]):\n",
    "                    print(\"Stop criterion met after {} iterations\".format(counter))\n",
    "                    break\n",
    "\n",
    "        #when we sample from the negatives we only take 17 nucleotide chunks from each site\n",
    "        \n",
    "        if args['<sampling>'] == 'space':\n",
    "            for site in neg_site_training:\n",
    "                \n",
    "                number_of_chunkys = int(len(site) / 17) #length of neg site tells us the amount of 17 length chunks possible\n",
    "\n",
    "                for chunky in range(number_of_chunkys):\n",
    "                    slice = site[(chunky * 17):((chunky + 1) * 17)].seq\n",
    "                    if slice not in pos:\n",
    "                        if all([slice[4] == 'C', slice[5] == 'C', slice[9] == 'C']) == False:\n",
    "                            neuralnet.setin_n_exp_values(slice, autoencoder=False, negative=True)\n",
    "                            neuralnet.forward_propogation()\n",
    "                            neuralnet.backward_propogation()\n",
    "                            neuralnet.weight_bias_renew()\n",
    "                            pos_counter += 1\n",
    "\n",
    "                        else:\n",
    "                            print(slice)\n",
    "\n",
    "                    #quick check to make sure that we've finished going thru the positives yet\n",
    "                    if pos_counter == len(pos_site_training):\n",
    "                        for pos_site in pos_site_training:\n",
    "                            neuralnet.setin_n_exp_values(pos_site, autoencoder=False, negative=False)\n",
    "                            neuralnet.forward_propogation()\n",
    "                            neuralnet.backward_propogation()\n",
    "                            neuralnet.weight_bias_renew()\n",
    "\n",
    "                        pos_counter = 0\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                greatestdelta_1 = neuralnet.errors_mat1.max()\n",
    "                smallestdelta_1 = neuralnet.errors_mat1.min()\n",
    "                greatestdelta_2 = neuralnet.errors_mat2.max()\n",
    "                smallestdelta_2 = neuralnet.errors_mat2.min()\n",
    "\n",
    "                if any([greatestdelta_1 < 0.00000000001 and greatestdelta_1 > 0,\n",
    "                        smallestdelta_1 > -.00000000001 and smallestdelta_1 < 0]) and any(\n",
    "                    [greatestdelta_2 < 0.00000000001 and greatestdelta_2 > 0,\n",
    "                     smallestdelta_2 > -0.00000000001 and smallestdelta_2 < 0]):\n",
    "                    print(\"Stop criterion met after {} iterations\".format(counter))\n",
    "                    break\n",
    "\n",
    "        # taken each partition and trained model\n",
    "        print(\"Performing Cross-validation\")\n",
    "\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "\n",
    "        \n",
    "        # Return the sets of positives and negatives from the x-validation\n",
    "        \n",
    "        print(\"Negative cross-validation set...\")\n",
    "        counter = 0\n",
    "        for site in neg_cross_validation_set:\n",
    "            for slice in range(len(site) - 16):\n",
    "                neuralnet.setin_n_exp_values(site[slice:slice + 17].seq, autoencoder=False, negative=True)\n",
    "                neuralnet.forward_propogation()\n",
    "                neg_list.append(neuralnet.output_layer_output)\n",
    "            counter += 1\n",
    "            print(\"Negative cross-validation: {}/{} completed...\".format(counter, len(neg_cross_validation_set)))\n",
    "            break\n",
    "\n",
    "        print(\"Positive cross-validation set...\")\n",
    "        for site in pos_cross_validation_set:\n",
    "            neuralnet.setin_n_exp_values(site, autoencoder=False)\n",
    "            neuralnet.forward_propogation()\n",
    "            pos_list.append(neuralnet.output_layer_output)\n",
    "\n",
    "        print('Positive avg: {}'.format(sum(pos_list) / len(pos_list)))\n",
    "        print('Negative avg: {}'.format(sum(neg_list) / len(neg_list)))\n",
    "        print(neuralnet.bias_mat1)\n",
    "        print(neuralnet.bias_mat2)\n",
    "\n",
    "        # Output the coneuralnetection matrices with greatest separation between the average positive and negative scores\n",
    "        if ((sum(pos_list) / len(pos_list)) - (sum(neg_list) / len(neg_list))) > separation:\n",
    "            np.savetxt('cnx_matrix_1.csv', neuralnet.bias_mat1, delimiter=',')\n",
    "            np.savetxt('cnx_matrix_2.csv', neuralnet.bias_mat2, delimiter=',')\n",
    "            separation = (sum(pos_list) / len(pos_list)) - (sum(neg_list) / len(neg_list))\n",
    "\n",
    "\n",
    "# A simple definition of the autoencoder that uses those same neuralnet parameters\n",
    "def autoencoder():\n",
    "  \n",
    "    neuralnet = neural_network()\n",
    "    neuralnet.setin_n_exp_values('GA', autoencoder=True)\n",
    "    neuralnet.initialize_values()\n",
    "\n",
    "    # Stop criterion\n",
    "    finished_working = False\n",
    "\n",
    "    while finished_working == False:\n",
    "        neuralnet.forward_propogation()\n",
    "        neuralnet.backward_propogation()\n",
    "        neuralnet.weight_bias_renew()\n",
    "\n",
    "        greatestdelta_1 = neuralnet.errors_mat1.max()\n",
    "        smallestdelta_1 = neuralnet.errors_mat1.min()\n",
    "        greatestdelta_2 = neuralnet.errors_mat2.max()\n",
    "        smallestdelta_2 = neuralnet.errors_mat2.min()\n",
    "\n",
    "        if any([greatestdelta_1 < 0.00001 and greatestdelta_1 > 0,\n",
    "                smallestdelta_1 > -.00001 and smallestdelta_1 < 0]) or any(\n",
    "            [greatestdelta_2 < 0.00001 and greatestdelta_2 > 0,\n",
    "             smallestdelta_2 > -0.00001 and smallestdelta_2 < 0]):\n",
    "            finished_working = True\n",
    "\n",
    "    print(neuralnet.output_layer_output)\n",
    "\n",
    "def test():\n",
    "    test_sequences = open('data/rap1-lieb-test.txt')\n",
    "    neuralnet = neural_network(68, 23, 1)\n",
    "    neuralnet.bias_mat1 = np.loadtxt('cnx_matrix_1.csv', delimiter=',')\n",
    "    neuralnet.bias_mat2 = np.loadtxt('cnx_matrix_2.csv', delimiter=',')\n",
    "\n",
    "    neuralnet_outputs = open('neuralnet_predictions.txt', 'w')\n",
    "\n",
    "    for test_seq in test_sequences:\n",
    "        neuralnet.setin_n_exp_values(test_seq.strip())\n",
    "        neuralnet.forward_propogation()\n",
    "        neuralnet_outputs.write('{}\\t{}\\n'.format(test_seq.strip(), neuralnet.output_layer_output[0]))\n",
    "\n",
    "    neuralnet_outputs.close()\n",
    "\n",
    "if __name__ == 'train':\n",
    "    import docopt\n",
    "    import numpy as np\n",
    "    from Bio import SeqIO\n",
    "    import copy\n",
    "    from .neuralnetfxns import neural_network\n",
    "\n",
    "    args = docopt.docopt(__doc__)\n",
    "\n",
    "    if args['autoencoder']:\n",
    "        autoencoder()\n",
    "\n",
    "    if args['testme']:\n",
    "        testme()\n",
    "\n",
    "    if args['test']:\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run 4th\n",
    "testme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run 5th\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "training_set = np.zeros((8,8), dtype = np.int)\n",
    "for i in range(0,8):\n",
    "    training_set[i][i] = 1\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
